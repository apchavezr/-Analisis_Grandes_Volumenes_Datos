{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/Pipeline_ETL_Contenedores_Kubernetes_AWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ac0bb2",
      "metadata": {
        "id": "f4ac0bb2"
      },
      "source": [
        "\n",
        "# 🧪 Ejemplo: Pipeline ETL escalable con contenedores en AWS + Kubernetes\n",
        "\n",
        "Este notebook documenta el diseño de un pipeline ETL distribuido, donde un contenedor ejecuta la transformación de datos almacenados en Amazon S3, y Kubernetes se encarga de escalar las instancias (pods) automáticamente según el volumen de datos.\n",
        "\n",
        "La lógica puede adaptarse también a Google Cloud Platform (GCP) usando GCS y GKE.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17668f48",
      "metadata": {
        "id": "17668f48"
      },
      "source": [
        "\n",
        "## 📁 Código del contenedor ETL en Python\n",
        "\n",
        "El siguiente script realiza la transformación de datos descargados desde S3 y vuelve a guardar los resultados en el bucket:\n",
        "\n",
        "```python\n",
        "# etl_transform.py\n",
        "\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import os\n",
        "\n",
        "def main():\n",
        "    s3 = boto3.client('s3')\n",
        "    bucket = os.environ['S3_BUCKET']\n",
        "    key = os.environ['S3_KEY']\n",
        "\n",
        "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "    df = pd.read_csv(obj['Body'])\n",
        "\n",
        "    # Transformación: convertir a mayúsculas una columna\n",
        "    df['processed'] = df[df.columns[0]].apply(lambda x: str(x).upper())\n",
        "    df.to_csv('/tmp/output.csv', index=False)\n",
        "\n",
        "    s3.upload_file('/tmp/output.csv', bucket, 'output/output.csv')\n",
        "    print(\"ETL completado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc1e354",
      "metadata": {
        "id": "5dc1e354"
      },
      "source": [
        "\n",
        "## 🐳 Dockerfile del contenedor ETL\n",
        "\n",
        "```dockerfile\n",
        "FROM python:3.9\n",
        "\n",
        "RUN pip install pandas boto3\n",
        "\n",
        "COPY etl_transform.py /app/\n",
        "WORKDIR /app\n",
        "\n",
        "CMD [\"python\", \"etl_transform.py\"]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd447f3",
      "metadata": {
        "id": "ddd447f3"
      },
      "source": [
        "\n",
        "## ☸️ YAML de despliegue en Kubernetes\n",
        "\n",
        "El contenedor puede ejecutarse como un `Job` en Kubernetes. La configuración es:\n",
        "\n",
        "```yaml\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: etl-job\n",
        "spec:\n",
        "  template:\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: etl\n",
        "        image: <aws_account_id>.dkr.ecr.<region>.amazonaws.com/etl-image:latest\n",
        "        env:\n",
        "        - name: S3_BUCKET\n",
        "          value: \"mi-bucket-etl\"\n",
        "        - name: S3_KEY\n",
        "          value: \"input/datos.csv\"\n",
        "      restartPolicy: Never\n",
        "  backoffLimit: 4\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438965ea",
      "metadata": {
        "id": "438965ea"
      },
      "source": [
        "\n",
        "## 🔄 Escalamiento automático\n",
        "\n",
        "Para escalar el número de pods según el uso de CPU:\n",
        "\n",
        "```bash\n",
        "kubectl autoscale deployment etl-job --cpu-percent=70 --min=1 --max=10\n",
        "```\n",
        "\n",
        "Esto permite que Kubernetes despliegue más contenedores si el volumen de datos o la carga computacional lo requiere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d3f597",
      "metadata": {
        "id": "37d3f597"
      },
      "source": [
        "\n",
        "## 📌 Conclusión\n",
        "\n",
        "Este pipeline demuestra cómo integrar contenedores, almacenamiento en la nube y orquestación con Kubernetes para construir soluciones escalables en Big Data. Es una arquitectura que puede implementarse tanto en AWS (EKS + S3) como en GCP (GKE + GCS).\n",
        "\n",
        "Este enfoque promueve elasticidad, modularidad y eficiencia operacional en el procesamiento de grandes volúmenes de datos.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}