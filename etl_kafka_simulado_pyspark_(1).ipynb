{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/etl_kafka_simulado_pyspark_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfaoZ4GyehLj"
      },
      "source": [
        "# Transformación de datos con PySpark\n",
        "Simulación de datos tipo Kafka y procesamiento distribuido con PySpark en Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK485Sc0ehLk"
      },
      "source": [
        "## Paso 1: Instalación de PySpark\n",
        "Se instala la biblioteca `pyspark`, que es la interfaz de Apache Spark para Python. Este paso es necesario en Colab, ya que el entorno no la tiene instalada por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGWnURGfehLl",
        "outputId": "7b2e8bc0-2e8b-44fb-ba28-fd28a2144579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Instalar PySpark en Colab\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWRrZeaTehLl"
      },
      "source": [
        "## Paso 2: Inicialización de la sesión Spark\n",
        "Aquí se crea una instancia de `SparkSession`, la puerta de entrada para utilizar Spark con DataFrames. Esta sesión nos permitirá realizar operaciones distribuidas sobre conjuntos de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EW6Y4_HdehLm"
      },
      "outputs": [],
      "source": [
        "# Inicializar sesión de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL_simulado\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3UrU2LehLm"
      },
      "source": [
        "## Paso 3: Simulación de datos provenientes de Kafka\n",
        "En lugar de conectar directamente a Kafka (que no está disponible en Colab), se crea un archivo JSON local con una estructura similar a los mensajes que Kafka transmitiría.\n",
        "El archivo contiene eventos con distintos tipos, como `compra` y `consulta`, con valores numéricos asociados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aSRS-DveehLm"
      },
      "outputs": [],
      "source": [
        "# Simular datos tipo Kafka (JSON)\n",
        "data_json = \"\"\"\n",
        "[\n",
        "  {\"user_id\": \"u001\", \"event_type\": \"compra\", \"timestamp\": \"2025-04-21T10:00:00\", \"value\": 1500},\n",
        "  {\"user_id\": \"u002\", \"event_type\": \"consulta\", \"timestamp\": \"2025-04-21T10:02:00\", \"value\": 0},\n",
        "  {\"user_id\": \"u003\", \"event_type\": \"compra\", \"timestamp\": \"2025-04-21T10:03:00\", \"value\": 950},\n",
        "  {\"user_id\": \"u004\", \"event_type\": \"compra\", \"timestamp\": \"2025-04-21T10:05:00\", \"value\": 3000}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"eventos_kafka.json\", \"w\") as f:\n",
        "    f.write(data_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kZ4BBQfehLn"
      },
      "source": [
        "## Paso 4: Lectura del archivo JSON\n",
        "Se usa `spark.read.json()` para cargar el archivo JSON en un DataFrame. Esto simula la lectura de un flujo de datos de Kafka.\n",
        "### Salida esperada:\n",
        "Una tabla con columnas `user_id`, `event_type`, `timestamp`, `value`, que representa el conjunto original de eventos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vthI71PBehLn",
        "outputId": "18a94434-1257-4491-a8a0-ffc9edf48092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-------------------+-------+-----+\n",
            "|_corrupt_record|event_type|          timestamp|user_id|value|\n",
            "+---------------+----------+-------------------+-------+-----+\n",
            "|              [|      NULL|               NULL|   NULL| NULL|\n",
            "|           NULL|    compra|2025-04-21T10:00:00|   u001| 1500|\n",
            "|           NULL|  consulta|2025-04-21T10:02:00|   u002|    0|\n",
            "|           NULL|    compra|2025-04-21T10:03:00|   u003|  950|\n",
            "|           NULL|    compra|2025-04-21T10:05:00|   u004| 3000|\n",
            "|              ]|      NULL|               NULL|   NULL| NULL|\n",
            "+---------------+----------+-------------------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Leer JSON simulado\n",
        "df_eventos = spark.read.json(\"eventos_kafka.json\")\n",
        "df_eventos.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvFw3v6ehLn"
      },
      "source": [
        "## Paso 5: Filtrado de eventos relevantes\n",
        "Se aplica un filtro para conservar únicamente los eventos de tipo `compra` cuyo valor sea superior a 1000. Esto es una forma básica de limpieza de datos.\n",
        "### Salida esperada:\n",
        "Un subconjunto del DataFrame original, que solo incluye las compras importantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH4EvQSZehLn",
        "outputId": "d54ca0d3-451d-4ee5-e014-33176f35a657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-------------------+-------+-----+\n",
            "|_corrupt_record|event_type|          timestamp|user_id|value|\n",
            "+---------------+----------+-------------------+-------+-----+\n",
            "|           NULL|    compra|2025-04-21T10:00:00|   u001| 1500|\n",
            "|           NULL|    compra|2025-04-21T10:05:00|   u004| 3000|\n",
            "+---------------+----------+-------------------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtrar eventos de tipo 'compra' con valor > 1000\n",
        "df_filtrado = df_eventos.filter((df_eventos.event_type == \"compra\") & (df_eventos.value > 1000))\n",
        "df_filtrado.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkn4MqnhehLo"
      },
      "source": [
        "## Paso 6: Clasificación por categorías\n",
        "Se agrega una nueva columna `categoria` que clasifica las compras según su valor:\n",
        "- `alta`: compras mayores o iguales a 3000\n",
        "- `media`: compras entre 1500 y 2999\n",
        "- `baja`: compras de 1000 a 1499 (aunque no se genera en este ejemplo)\n",
        "### Salida esperada:\n",
        "Un DataFrame enriquecido con una columna adicional que indica la categoría de la compra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6a1zwD7ehLo",
        "outputId": "ae4ba102-386d-40ef-c73c-e1183e00bfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "|_corrupt_record|event_type|          timestamp|user_id|value|categoria|\n",
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "|           NULL|    compra|2025-04-21T10:00:00|   u001| 1500|    media|\n",
            "|           NULL|    compra|2025-04-21T10:05:00|   u004| 3000|     alta|\n",
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Clasificar por categoría\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_enriquecido = df_filtrado.withColumn(\n",
        "    \"categoria\",\n",
        "    when(df_filtrado.value >= 3000, \"alta\")\n",
        "    .when(df_filtrado.value >= 1500, \"media\")\n",
        "    .otherwise(\"baja\")\n",
        ")\n",
        "\n",
        "df_enriquecido.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxlSOU9OehLo"
      },
      "source": [
        "## Paso 7: Almacenamiento en formato Parquet\n",
        "El DataFrame resultante se guarda en el sistema de archivos local en formato Parquet. Este formato es columnar y altamente eficiente, ideal para grandes volúmenes de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A3zBuJmYehLo"
      },
      "outputs": [],
      "source": [
        "# Guardar en formato Parquet\n",
        "df_enriquecido.write.mode(\"overwrite\").parquet(\"compras_filtradas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL8QeZSeehLo"
      },
      "source": [
        "## Paso 8: Lectura de verificación\n",
        "Se lee el archivo Parquet guardado anteriormente para verificar que los datos fueron correctamente transformados y almacenados.\n",
        "### Salida esperada:\n",
        "Un DataFrame que contiene únicamente las compras filtradas, con su respectiva categoría."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKEk575eehLo",
        "outputId": "0667eac3-f112-4bb9-f2d9-7be12ea47857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "|_corrupt_record|event_type|          timestamp|user_id|value|categoria|\n",
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "|           NULL|    compra|2025-04-21T10:00:00|   u001| 1500|    media|\n",
            "|           NULL|    compra|2025-04-21T10:05:00|   u004| 3000|     alta|\n",
            "+---------------+----------+-------------------+-------+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Leer los datos almacenados para verificación\n",
        "df_verificacion = spark.read.parquet(\"/tmp/compras_filtradas\")\n",
        "df_verificacion.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQyjgYeDehLp"
      },
      "source": [
        "## Paso 9: Finalización de la sesión\n",
        "Se cierra la sesión de Spark para liberar los recursos utilizados en el procesamiento distribuido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dOjs-2PdehLp"
      },
      "outputs": [],
      "source": [
        "# Cierre de sesión\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}