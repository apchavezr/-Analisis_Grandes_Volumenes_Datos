{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/Ejercicio_MapReduce_vs_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a8c051",
      "metadata": {
        "id": "d4a8c051"
      },
      "source": [
        "# Ejercicio práctico: Simulación y análisis comparativo de ejecución MapReduce vs. Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84254b36",
      "metadata": {
        "id": "84254b36"
      },
      "source": [
        "Este ejercicio tiene como propósito que el estudiante compare de forma práctica los enfoques de procesamiento de datos de Hadoop MapReduce y Apache Spark, analizando sus diferencias en cuanto a modelo de programación, rendimiento y escalabilidad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5575c81e",
      "metadata": {
        "id": "5575c81e"
      },
      "source": [
        "## Parte 1 – Simulación del proceso MapReduce (en Python local)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a106f598",
      "metadata": {
        "id": "a106f598"
      },
      "source": [
        "Ejecute el siguiente código para simular un conteo de palabras usando el paradigma MapReduce en Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae4a589a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae4a589a",
        "outputId": "a694766e-bbca-4540-89b7-ae0194513eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados MapReduce simulados:\n",
            "big: 3\n",
            "data: 2\n",
            "requiere: 1\n",
            "procesamiento: 1\n",
            "distribuido: 1\n",
            "y: 1\n",
            "spark: 2\n",
            "acelera: 1\n",
            "procesos: 1\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "texto = \"big data requiere procesamiento distribuido y spark acelera procesos big data big spark\"\n",
        "\n",
        "# Map: convierte en pares (palabra, 1)\n",
        "map_output = [(palabra, 1) for palabra in texto.split()]\n",
        "\n",
        "# Reduce: agrupa y suma\n",
        "reduce_output = defaultdict(int)\n",
        "for palabra, cuenta in map_output:\n",
        "    reduce_output[palabra] += cuenta\n",
        "\n",
        "# Resultados\n",
        "print(\"Resultados MapReduce simulados:\")\n",
        "for palabra, cuenta in reduce_output.items():\n",
        "    print(f\"{palabra}: {cuenta}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc2ad84",
      "metadata": {
        "id": "7cc2ad84"
      },
      "source": [
        "## Parte 2 – Procesamiento con PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a97e1f1",
      "metadata": {
        "id": "6a97e1f1"
      },
      "source": [
        "Instale PySpark si está ejecutando este notebook en Google Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9ef9c166",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef9c166",
        "outputId": "f6580075-1d3e-4114-a6e4-60800f38f775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee97c86d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee97c86d",
        "outputId": "92ef8408-df68-4073-87b1-5dc4291377d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados con PySpark RDD:\n",
            "big: 3\n",
            "requiere: 1\n",
            "procesamiento: 1\n",
            "distribuido: 1\n",
            "data: 2\n",
            "y: 1\n",
            "spark: 2\n",
            "acelera: 1\n",
            "procesos: 1\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Crear el contexto de Spark\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "texto = \"big data requiere procesamiento distribuido y spark acelera procesos big data big spark\"\n",
        "palabras = sc.parallelize(texto.split())\n",
        "\n",
        "conteo = palabras.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"Resultados con PySpark RDD:\")\n",
        "for palabra, cuenta in conteo.collect():\n",
        "    print(f\"{palabra}: {cuenta}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b02c513",
      "metadata": {
        "id": "3b02c513"
      },
      "source": [
        "## Parte 3 – Reflexión técnica (para responder en el cuaderno)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3fdc9c",
      "metadata": {
        "id": "fb3fdc9c"
      },
      "source": [
        "Responda brevemente:\n",
        "\n",
        "1. ¿Qué diferencias observó en los enfoques de programación entre el modelo MapReduce simulado y Spark?\n",
        "2. ¿Qué ventajas ofrece Spark frente a MapReduce en cuanto a escalabilidad y rendimiento?\n",
        "3. Si trabajara con un conjunto de datos 100 veces mayor, ¿cuál modelo sería más conveniente implementar en un clúster y por qué?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd7ced1",
      "metadata": {
        "id": "6fd7ced1"
      },
      "source": [
        "## Recursos recomendados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc6d9482",
      "metadata": {
        "id": "cc6d9482"
      },
      "source": [
        "- Zaharia et al. (2012). *Resilient distributed datasets*: https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf  \n",
        "- Video de comparación técnica: https://www.youtube.com/watch?v=EN3jkhJ4GgE  \n",
        "- Curso gratuito de Apache Spark: https://www.databricks.com/learn/training/free/apache-spark-scap\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}