{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/Notebook_Tutorial_PySpark_Riesgo_Crediticio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYecyFW92kV"
      },
      "source": [
        "# Guía práctica: Clasificación de riesgo crediticio con PySpark y Spark ML\n",
        "\n",
        "Este cuaderno tiene como objetivo guiar al estudiante paso a paso en la construcción de un pipeline de aprendizaje automático utilizando PySpark, para predecir el riesgo de incumplimiento de pago por parte de clientes de una entidad financiera. Se utiliza el conjunto de datos real 'Default of Credit Card Clients Dataset' disponible en Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLQrmGC592kX"
      },
      "source": [
        "## Paso 1: Instalar PySpark (solo si está en Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWarp6Do92kY",
        "outputId": "b97b7efa-a0d0-4367-a099-d94c22698e7d"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CinsLMVl92kZ"
      },
      "source": [
        "## Paso 2: Importar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywTjLM5C92ka"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzicFx3A92ka"
      },
      "source": [
        "## Paso 3: Crear una sesión de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7FKiPfb92ka"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TutorialRiesgoCrediticio\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2tuwuSf92ka"
      },
      "source": [
        "## Paso 4: Cargar el archivo de datos\n",
        "\n",
        "Asegúrese de haber descargado y subido el archivo `UCI_Credit_Card.csv` desde Kaggle antes de ejecutar esta celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfNMtodw92kb",
        "outputId": "996723d8-4b99-48b5-841b-07b59b029ebe"
      },
      "source": [
        "df = spark.read.csv(\"UCI_Credit_Card.csv\", header=True, inferSchema=True)\n",
        "df = df.withColumnRenamed(\"default.payment.next.month\", \"label\")\n",
        "df.select(\"LIMIT_BAL\", \"AGE\", \"EDUCATION\", \"MARRIAGE\", \"SEX\", \"label\").show(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+---------+--------+---+-----+\n",
            "|LIMIT_BAL|AGE|EDUCATION|MARRIAGE|SEX|label|\n",
            "+---------+---+---------+--------+---+-----+\n",
            "|  20000.0| 24|        2|       1|  2|    1|\n",
            "| 120000.0| 26|        2|       2|  2|    1|\n",
            "|  90000.0| 34|        2|       2|  2|    0|\n",
            "|  50000.0| 37|        2|       1|  2|    0|\n",
            "|  50000.0| 57|        2|       1|  1|    0|\n",
            "+---------+---+---------+--------+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL563KAH92kb"
      },
      "source": [
        "## Paso 5: Transformar variables categóricas\n",
        "Convertimos las variables EDUCATION, MARRIAGE y SEX a índices numéricos para que puedan ser utilizadas por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbLw0EIq92kb"
      },
      "source": [
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col+\"_idx\")\n",
        "    for col in [\"EDUCATION\", \"MARRIAGE\", \"SEX\"]\n",
        "]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJMOaPP392kb"
      },
      "source": [
        "## Paso 6: Combinar variables en un vector de características\n",
        "Utilizamos `VectorAssembler` para agrupar las variables predictoras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMpJfPCI92kc"
      },
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"LIMIT_BAL\", \"AGE\"] + [col+\"_idx\" for col in [\"EDUCATION\", \"MARRIAGE\", \"SEX\"]],\n",
        "    outputCol=\"features_raw\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CquXD7Ek92kc"
      },
      "source": [
        "## Paso 7: Escalar las características\n",
        "Para mejorar la estabilidad numérica del modelo, escalamos las características con `StandardScaler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGHld1A992kc"
      },
      "source": [
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exiuJyL592kc"
      },
      "source": [
        "## Paso 8: Definir modelo y pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sbxe3KD92kd"
      },
      "source": [
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler, lr])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbo8PuE92kd"
      },
      "source": [
        "## Paso 9: Entrenar el modelo y hacer predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA10ljWd92kd",
        "outputId": "ddca10e5-7c94-4996-dcbd-5b1d135b9722"
      },
      "source": [
        "model = pipeline.fit(df)\n",
        "predictions = model.transform(df)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    1|       0.0|[0.69400241111226...|\n",
            "|    1|       0.0|[0.78484208412147...|\n",
            "|    0|       0.0|[0.75881277160846...|\n",
            "|    0|       0.0|[0.69929370572321...|\n",
            "|    0|       0.0|[0.63602236063087...|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYGRzBG892kd"
      },
      "source": [
        "## Paso 10: Evaluar el modelo\n",
        "Usamos el área bajo la curva ROC para evaluar la capacidad predictiva del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EIMS6d992kd",
        "outputId": "6f51f552-8421-488a-c607-bb0436fd05c3"
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Área bajo la curva (AUC): {auc:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Área bajo la curva (AUC): 0.6223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx_dAZNx92ke"
      },
      "source": [
        "## Conclusión\n",
        "Este ejercicio muestra cómo construir un flujo completo de aprendizaje automático escalable utilizando PySpark y Spark ML. Se ha implementado la predicción de riesgo crediticio sobre un conjunto de datos real, incluyendo el preprocesamiento, ensamblaje, escalado, modelado y evaluación del rendimiento."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}