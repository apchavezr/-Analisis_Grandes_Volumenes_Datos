{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/Kafka_PySpark_Hive_Airflow_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hySUDI1FiUNm"
      },
      "source": [
        "# Flujo distribuido: Kafka + PySpark + Hive + Airflow\n",
        "\n",
        "Este notebook presenta un ejemplo simplificado del flujo de procesamiento distribuido utilizando PySpark para consumir datos desde Apache Kafka, transformarlos y almacenarlos en formato Parquet, simulando una integración con Apache Hive y Apache Airflow.\n",
        "\n",
        "**Componentes cubiertos:**\n",
        "- Kafka: simulado como fuente de datos.\n",
        "- PySpark: procesamiento de datos con Structured Streaming.\n",
        "- Hive: simulado mediante almacenamiento en formato Parquet.\n",
        "- Airflow: se describe cómo se orquestaría con DAG (fuera del notebook).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7YnEDzvHiUNo"
      },
      "outputs": [],
      "source": [
        "# Configuración inicial de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StringType, TimestampType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Kafka_PySpark_Integration\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBrtaIgGiUNp"
      },
      "source": [
        "## Esquema del mensaje desde Kafka (simulado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SzmCgSdAiUNq"
      },
      "outputs": [],
      "source": [
        "# Simulación del esquema de datos\n",
        "schema = StructType() \\\n",
        "    .add(\"sensor_id\", StringType()) \\\n",
        "    .add(\"zona\", StringType()) \\\n",
        "    .add(\"valor\", StringType()) \\\n",
        "    .add(\"timestamp\", TimestampType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPDjGomEiUNq"
      },
      "source": [
        "## Lectura desde Kafka (ejemplo real requeriría acceso a Kafka)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pPJLqmt7iUNq"
      },
      "outputs": [],
      "source": [
        "# kafkaDF = spark.readStream \\\n",
        "#     .format(\"kafka\") \\\n",
        "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "#     .option(\"subscribe\", \"eventos\") \\\n",
        "#     .load()\n",
        "\n",
        "# jsonDF = kafkaDF.selectExpr(\"CAST(value AS STRING)\")\n",
        "# dataDF = jsonDF.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PqX-RxyiUNr"
      },
      "source": [
        "## Simulación del DataFrame transformado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLQU1YoGiUNr",
        "outputId": "4fd11d80-53fb-4f6a-eb9f-731925dab85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----+--------------------+\n",
            "|sensor_id|     zona|valor|           timestamp|\n",
            "+---------+---------+-----+--------------------+\n",
            "|sensor-01|  Usaquén|   35|2025-04-21 18:09:...|\n",
            "|sensor-02|Chapinero|   48|2025-04-21 18:09:...|\n",
            "+---------+---------+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simulación local de datos\n",
        "from datetime import datetime\n",
        "data = [(\"sensor-01\", \"Usaquén\", \"35\", datetime.now()),\n",
        "        (\"sensor-02\", \"Chapinero\", \"48\", datetime.now())]\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Transformación: convertir valor a entero y agregar columna\n",
        "df_transformed = df.withColumn(\"valor\", col(\"valor\").cast(\"int\"))\n",
        "df_transformed.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFimHfRjiUNr"
      },
      "source": [
        "## Escritura en formato Parquet (para consumo posterior en Hive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UMDXoa9ViUNs"
      },
      "outputs": [],
      "source": [
        "df_transformed.write.mode(\"overwrite\").parquet(\"datos_sensores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7QHRT3iUNs"
      },
      "source": [
        "## Explicación paso a paso del flujo\n",
        "\n",
        "**1. Configuración inicial de PySpark:**\n",
        "Se crea una sesión de Spark con parámetros básicos. Esta sesión permite ejecutar operaciones distribuidas sobre conjuntos de datos de gran tamaño.\n",
        "\n",
        "**2. Definición del esquema:**\n",
        "Se especifica el formato de los datos que llegarán desde Kafka. Esto es clave para interpretar correctamente el contenido de los mensajes y realizar transformaciones posteriores.\n",
        "\n",
        "**3. Lectura desde Kafka:**\n",
        "Aunque se comenta en este ejemplo (por no tener acceso a un clúster Kafka en ejecución), se ilustra cómo se usaría `readStream` para consumir datos de un topic de Kafka. La transformación con `from_json` permite convertir los datos en estructuras tabulares.\n",
        "\n",
        "**4. Simulación del DataFrame:**\n",
        "Dado que no estamos en un entorno con Kafka en tiempo real, se crea manualmente un conjunto de datos simulados para representar los datos que llegarían desde Kafka. Se transforma la columna `valor` a entero para su análisis.\n",
        "\n",
        "**5. Escritura en Parquet:**\n",
        "Los datos transformados se almacenan en formato Parquet, optimizado para análisis posteriores en sistemas como Apache Hive. Hive puede leer directamente estos archivos y tratarlos como tablas para consultas SQL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyiLMXGZiUNs"
      },
      "source": [
        "## Conclusión\n",
        "\n",
        "Este flujo simulado permite comprender cómo una arquitectura moderna basada en tecnologías distribuidas puede gestionar datos en tiempo real desde su ingestión hasta su análisis.\n",
        "\n",
        "- **Kafka** actúa como buffer resiliente de datos en tiempo real.\n",
        "- **PySpark** procesa los datos con alta eficiencia mediante transformación distribuida.\n",
        "- **Hive** permite analizar los datos de forma declarativa y estructurada.\n",
        "- **Airflow** (aunque no se ejecuta directamente en este notebook) puede orquestar todo el pipeline y asegurar que cada componente se ejecute según lo planeado.\n",
        "\n",
        "Este flujo es aplicable en escenarios reales como monitoreo ambiental, análisis de tráfico o procesamiento de logs, y puede escalar horizontalmente en entornos como AWS EMR, Databricks o GCP Dataproc."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}