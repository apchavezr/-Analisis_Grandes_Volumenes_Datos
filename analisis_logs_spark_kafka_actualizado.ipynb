{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/analisis_logs_spark_kafka_actualizado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f17ebba",
      "metadata": {
        "id": "7f17ebba"
      },
      "source": [
        "# Análisis de Logs con Kafka, Spark Streaming y KMeans\n",
        "Este notebook guía el desarrollo de una práctica en la que se simula el procesamiento de logs con herramientas de Big Data.\n",
        "\n",
        "## Objetivos\n",
        "- Simular un flujo de logs con archivos `.log` generados artificialmente.\n",
        "- Implementar un pipeline local con Apache Kafka y Spark Streaming.\n",
        "- Aplicar clustering KMeans para identificar eventos anómalos.\n",
        "- Visualizar los resultados mediante Kibana.\n",
        "\n",
        "## Requisitos\n",
        "- Apache Kafka\n",
        "- Apache Spark con PySpark\n",
        "- Elasticsearch y Kibana\n",
        "- Python 3.8+\n",
        "- Librerías: `pyspark`, `pandas`, `matplotlib`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fe5e353e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe5e353e",
        "outputId": "e0d09617-6825-4681-8f32-7db2a5551b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo de logs generado.\n"
          ]
        }
      ],
      "source": [
        "# Simulación de logs artificiales\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generar_logs(ruta, n=1000):\n",
        "    niveles = ['INFO', 'WARN', 'ERROR']\n",
        "    servicios = ['auth', 'db', 'api', 'cache']\n",
        "    eventos = ['login', 'logout', 'timeout', 'access_denied', 'read', 'write']\n",
        "\n",
        "    with open(ruta, 'w') as f:\n",
        "        for _ in range(n):\n",
        "            tiempo = datetime.now() - timedelta(seconds=random.randint(0, 3600))\n",
        "            nivel = random.choice(niveles)\n",
        "            servicio = random.choice(servicios)\n",
        "            evento = random.choice(eventos)\n",
        "            linea = f\"{tiempo.isoformat()} [{nivel}] {servicio}: {evento}\\n\"\n",
        "            f.write(linea)\n",
        "\n",
        "generar_logs('logs_simulados.log', 1000)\n",
        "print(\"Archivo de logs generado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fefabd",
      "metadata": {
        "id": "b6fefabd"
      },
      "source": [
        "**Explicación**: Se inicia una sesión de Spark y se realiza la lectura del archivo `.log`. Luego, se aplican transformaciones para extraer los diferentes campos del registro, como el timestamp, nivel del log, servicio involucrado y evento. Esta estructura es necesaria para poder analizar y modelar los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a01f9dd",
      "metadata": {
        "id": "6a01f9dd"
      },
      "source": [
        "## Consumo de logs con Spark Streaming desde Kafka\n",
        "Este paso requiere que Kafka esté configurado y que los logs estén siendo enviados a un tópico llamado `logs-topic`.\n",
        "En Spark, se puede usar el siguiente código para consumir y transformar los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af9b4b1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af9b4b1f",
        "outputId": "0e11dce7-58c3-4f23-a4d2-7da7a92d1d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------+--------------------------+------+--------+-------------+\n",
            "|value                                                |timestamp                 |nivel |servicio|evento       |\n",
            "+-----------------------------------------------------+--------------------------+------+--------+-------------+\n",
            "|2025-04-21T19:43:58.964095 [INFO] auth: access_denied|2025-04-21T19:43:58.964095|[INFO]|auth:   |access_denied|\n",
            "|2025-04-21T19:54:08.964122 [INFO] auth: access_denied|2025-04-21T19:54:08.964122|[INFO]|auth:   |access_denied|\n",
            "|2025-04-21T20:05:10.964134 [WARN] db: read           |2025-04-21T20:05:10.964134|[WARN]|db:     |read         |\n",
            "|2025-04-21T20:32:22.964143 [WARN] auth: access_denied|2025-04-21T20:32:22.964143|[WARN]|auth:   |access_denied|\n",
            "|2025-04-21T20:07:16.964150 [INFO] auth: logout       |2025-04-21T20:07:16.964150|[INFO]|auth:   |logout       |\n",
            "+-----------------------------------------------------+--------------------------+------+--------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Código básico para iniciar Spark Streaming y consumir desde Kafka\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Log Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.text(\"logs_simulados.log\")\n",
        "logs_df = df.withColumn(\"timestamp\", split(col(\"value\"), \" \")[0]) \\\n",
        "            .withColumn(\"nivel\", split(col(\"value\"), \" \")[1]) \\\n",
        "            .withColumn(\"servicio\", split(col(\"value\"), \" \")[2]) \\\n",
        "            .withColumn(\"evento\", split(col(\"value\"), \": \")[1])\n",
        "\n",
        "logs_df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed8211c",
      "metadata": {
        "id": "3ed8211c"
      },
      "source": [
        "**Explicación**: Aquí se entrena el modelo KMeans con 3 clústeres (puede ajustarse según el análisis). Luego se aplica el modelo a los datos transformados para predecir a qué grupo pertenece cada registro. El resultado es útil para detectar patrones repetitivos o anómalos dentro del flujo de eventos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85cb91c7",
      "metadata": {
        "id": "85cb91c7"
      },
      "source": [
        "## Aplicación de KMeans para agrupar eventos similares\n",
        "Se realiza una codificación numérica básica y se aplica el algoritmo de clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0ecf8d43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ecf8d43",
        "outputId": "acd39e11-042e-4e7e-b913-98fe0037036d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------+----------+\n",
            "|value                                                |prediction|\n",
            "+-----------------------------------------------------+----------+\n",
            "|2025-04-21T19:43:58.964095 [INFO] auth: access_denied|2         |\n",
            "|2025-04-21T19:54:08.964122 [INFO] auth: access_denied|2         |\n",
            "|2025-04-21T20:05:10.964134 [WARN] db: read           |1         |\n",
            "|2025-04-21T20:32:22.964143 [WARN] auth: access_denied|2         |\n",
            "|2025-04-21T20:07:16.964150 [INFO] auth: logout       |0         |\n",
            "|2025-04-21T19:58:28.964156 [ERROR] db: write         |1         |\n",
            "|2025-04-21T19:47:32.964162 [INFO] cache: login       |0         |\n",
            "|2025-04-21T20:34:30.964171 [WARN] auth: access_denied|2         |\n",
            "|2025-04-21T19:49:23.964182 [WARN] db: read           |1         |\n",
            "|2025-04-21T20:06:23.964190 [INFO] api: access_denied |2         |\n",
            "+-----------------------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "indexer = StringIndexer(inputCols=[\"nivel\", \"servicio\", \"evento\"],\n",
        "                        outputCols=[\"nivel_index\", \"servicio_index\", \"evento_index\"])\n",
        "logs_indexed = indexer.fit(logs_df).transform(logs_df)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"nivel_index\", \"servicio_index\", \"evento_index\"], outputCol=\"features\")\n",
        "dataset = assembler.transform(logs_indexed)\n",
        "\n",
        "kmeans = KMeans(k=3, seed=1)\n",
        "modelo = kmeans.fit(dataset)\n",
        "predicciones = modelo.transform(dataset)\n",
        "predicciones.select(\"value\", \"prediction\").show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd68b01",
      "metadata": {
        "id": "abd68b01"
      },
      "source": [
        "## Conclusiones\n",
        "- La simulación de logs permite crear un entorno controlado para probar algoritmos de detección de anomalías.\n",
        "- Spark Streaming facilita la lectura y transformación de datos de forma escalable, adecuada para flujos en tiempo real.\n",
        "- El uso de clustering con KMeans permite identificar agrupaciones de eventos similares, lo que ayuda a clasificar errores o comportamientos inesperados.\n",
        "- Integrar este pipeline con herramientas de visualización como Kibana permite a los equipos de operaciones y seguridad monitorear los sistemas en tiempo real de forma visual e interactiva."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}