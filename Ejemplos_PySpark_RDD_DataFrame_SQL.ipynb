{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/-Analisis_Grandes_Volumenes_Datos/blob/main/Ejemplos_PySpark_RDD_DataFrame_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5863c78f",
      "metadata": {
        "id": "5863c78f"
      },
      "source": [
        "# Manejo de datos con PySpark: RDDs, DataFrames y consultas SQL distribuidas\n",
        "\n",
        "Este cuaderno presenta ejemplos ejecutables en PySpark para ilustrar el uso de Resilient Distributed Datasets (RDDs), DataFrames y Spark SQL. Está diseñado para que el estudiante pueda explorar y comparar estas tres formas de procesamiento de datos distribuidos en Apache Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6cac41da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cac41da",
        "outputId": "791d6a21-7735-47fc-f3e1-0f0a83a7f4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "435bea4c",
      "metadata": {
        "id": "435bea4c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder     .appName(\"Ejemplos PySpark\")     .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0ee2a394",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ee2a394",
        "outputId": "63dfaca3-2a2a-4432-97e0-7cb006a37f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado usando RDD:\n",
            "big: 3\n",
            "requiere: 1\n",
            "procesamiento: 1\n",
            "distribuido: 1\n",
            "data: 2\n",
            "y: 1\n",
            "spark: 2\n",
            "acelera: 1\n",
            "procesos: 1\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo 1: RDD - Conteo de palabras\n",
        "\n",
        "texto = \"big data requiere procesamiento distribuido y spark acelera procesos big data big spark\"\n",
        "palabras = sc.parallelize(texto.split())\n",
        "\n",
        "conteo = palabras.map(lambda palabra: (palabra, 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"Resultado usando RDD:\")\n",
        "for palabra, cuenta in conteo.collect():\n",
        "    print(f\"{palabra}: {cuenta}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7774fca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7774fca",
        "outputId": "2547271b-8cc8-4146-87fe-eb7f10b42d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "|nombre|edad|\n",
            "+------+----+\n",
            "|   Ana|  30|\n",
            "|  Luis|  25|\n",
            "|Carlos|  40|\n",
            "+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo 2: DataFrame - Creación desde lista de tuplas\n",
        "\n",
        "datos = [(\"Ana\", 30), (\"Luis\", 25), (\"Carlos\", 40)]\n",
        "columnas = [\"nombre\", \"edad\"]\n",
        "\n",
        "df = spark.createDataFrame(datos, columnas)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "47f7cf56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47f7cf56",
        "outputId": "a89cdbe6-0e32-45b2-afcb-db0c2d42690d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "|nombre|edad|\n",
            "+------+----+\n",
            "|Carlos|  40|\n",
            "+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo 3: Consultas SQL distribuidas\n",
        "\n",
        "df.createOrReplaceTempView(\"personas\")\n",
        "\n",
        "resultado = spark.sql(\"SELECT nombre, edad FROM personas WHERE edad > 30\")\n",
        "resultado.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a191c0b5",
      "metadata": {
        "id": "a191c0b5"
      },
      "source": [
        "# Reflexión\n",
        "\n",
        "- ¿Qué diferencias notó entre trabajar con RDDs y DataFrames?\n",
        "- ¿Cuál estructura considera más apropiada para un conjunto de datos grande con múltiples columnas?\n",
        "- ¿Qué ventajas aporta el uso de SQL para el análisis exploratorio en entornos distribuidos?\n",
        "\n",
        "Puede documentar sus respuestas al final de este cuaderno.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}